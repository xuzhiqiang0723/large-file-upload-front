/* eslint-disable unicorn/prefer-add-event-listener */
/* eslint-disable unicorn/prefer-blob-reading-methods */
import { computed, reactive, ref } from 'vue'

import SparkMD5 from 'spark-md5'

interface S3ChunkInfo {
  index: number
  partNumber: number
  start: number
  end: number
  blob: Blob
  uploaded: boolean
  progress: number
  retryCount: number
  etag?: string
  uploadStartTime?: number
  uploadEndTime?: number
  chunkHash?: string // æ·»åŠ åˆ†ç‰‡å“ˆå¸Œ
  abortController?: AbortController // æ·»åŠ å–æ¶ˆæ§åˆ¶å™¨
}

interface S3UploadOptions {
  chunkSize?: number
  concurrent?: number
  retryTimes?: number
  baseUrl?: string
  headers?: Record<string, string>
  hashChunkSize?: number
}

interface S3UploadSession {
  sessionId: string
  uploadId: string
  objectName: string
  fileName: string
  fileSize: number
  totalChunks: number
  uploadedParts?: number
  progress?: number
  parts?: Array<{ etag: string; partNumber: number }>
}

interface S3UploadProgress {
  loaded: number
  total: number
  percentage: number
}

interface S3SpeedInfo {
  current: number
  average: number
  peak: number
  lastUpdate: number
}

interface S3NetworkStats {
  uploadedBytes: number
  totalBytes: number
  startTime: number
  lastMeasureTime: number
  lastMeasureBytes: number
  speedHistory: number[]
}

interface S3CheckUploadResponse {
  success: boolean
  fileExists: boolean
  isComplete: boolean
  message: string
  url?: string
  session?: S3UploadSession
}

export function useS3Upload(options: S3UploadOptions = {}) {
  const {
    chunkSize = 5 * 1024 * 1024, // S3æ¨è5MBåˆ†ç‰‡
    concurrent = 3,
    retryTimes = 3,
    baseUrl = '/api/s3/upload',
    headers = {},
    hashChunkSize = 2 * 1024 * 1024
  } = options

  // å“åº”å¼çŠ¶æ€
  const isUploading = ref(false)
  const isPaused = ref(false)
  const isCompleted = ref(false)
  const isCalculatingHash = ref(false)
  const isCheckingUpload = ref(false)
  const isInitializing = ref(false)
  const currentFile = ref<File | null>(null)
  const fileHash = ref('')
  const chunks = ref<S3ChunkInfo[]>([])
  const uploadSession = ref<null | S3UploadSession>(null)

  const uploadProgress = reactive<S3UploadProgress>({
    loaded: 0,
    total: 0,
    percentage: 0
  })

  const hashProgress = reactive({
    loaded: 0,
    total: 0,
    percentage: 0
  })

  const speedInfo = reactive<S3SpeedInfo>({
    current: 0,
    average: 0,
    peak: 0,
    lastUpdate: 0
  })

  const networkStats = reactive<S3NetworkStats>({
    uploadedBytes: 0,
    totalBytes: 0,
    startTime: 0,
    lastMeasureTime: 0,
    lastMeasureBytes: 0,
    speedHistory: []
  })

  const isSecondTransfer = ref(false)
  const resumeInfo = ref<any>(null)
  const executingChunks = ref<Set<number>>(new Set()) // æ­£åœ¨æ‰§è¡Œçš„åˆ†ç‰‡ç´¢å¼•

  // è®¡ç®—å±æ€§
  const uploadedChunks = computed(() => chunks.value.filter(chunk => chunk.uploaded))

  const remainingChunks = computed(() => chunks.value.filter(chunk => !chunk.uploaded))

  const totalChunks = computed(() => chunks.value.length)

  const uploadedSize = computed(() => uploadedChunks.value.reduce((sum, chunk) => sum + (chunk.end - chunk.start), 0))

  // å·¥å…·å‡½æ•°
  const formatSpeed = (bytesPerSecond: number): string => {
    if (bytesPerSecond === 0) return '0 B/s'
    if (bytesPerSecond >= 1024 * 1024 * 1024) {
      return `${(bytesPerSecond / (1024 * 1024 * 1024)).toFixed(2)} GB/s`
    } else if (bytesPerSecond >= 1024 * 1024) {
      return `${(bytesPerSecond / (1024 * 1024)).toFixed(2)} MB/s`
    } else if (bytesPerSecond >= 1024) {
      return `${(bytesPerSecond / 1024).toFixed(2)} KB/s`
    } else {
      return `${bytesPerSecond.toFixed(0)} B/s`
    }
  }

  const calculateRemainingTime = (): string => {
    if (speedInfo.average <= 0) return 'è®¡ç®—ä¸­...'
    const remainingBytes = uploadProgress.total - uploadProgress.loaded
    const remainingSeconds = remainingBytes / speedInfo.average

    if (remainingSeconds > 3600) {
      const hours = Math.floor(remainingSeconds / 3600)
      const minutes = Math.floor((remainingSeconds % 3600) / 60)
      return `${hours}å°æ—¶${minutes}åˆ†é’Ÿ`
    } else if (remainingSeconds > 60) {
      const minutes = Math.floor(remainingSeconds / 60)
      const seconds = Math.floor(remainingSeconds % 60)
      return `${minutes}åˆ†${seconds}ç§’`
    } else {
      return `${Math.floor(remainingSeconds)}ç§’`
    }
  }

  // è®¡ç®—åˆ†ç‰‡å“ˆå¸Œ
  const calculateChunkHash = async (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const spark = new SparkMD5.ArrayBuffer()
      const fileReader = new FileReader()

      fileReader.addEventListener('load', e => {
        try {
          if (e.target?.result) {
            spark.append(e.target.result as ArrayBuffer)
            const hash = spark.end()
            resolve(hash)
          }
        } catch {
          reject(new Error('åˆ†ç‰‡å“ˆå¸Œè®¡ç®—å¤±è´¥'))
        }
      })

      fileReader.onerror = () => {
        reject(new Error('åˆ†ç‰‡è¯»å–å¤±è´¥'))
      }

      fileReader.readAsArrayBuffer(blob)
    })
  }

  // ç½‘ç»œé€Ÿåº¦ç»Ÿè®¡
  const updateNetworkStats = (uploadedBytes: number) => {
    const now = Date.now()

    if (networkStats.startTime === 0) {
      networkStats.startTime = now
      networkStats.lastMeasureTime = now
      networkStats.lastMeasureBytes = uploadedBytes
      return
    }

    const timeDiff = now - networkStats.lastMeasureTime
    if (timeDiff >= 1000) {
      const bytesDiff = uploadedBytes - networkStats.lastMeasureBytes
      const currentSpeed = bytesDiff / (timeDiff / 1000)

      speedInfo.current = currentSpeed
      speedInfo.lastUpdate = now

      networkStats.speedHistory.push(currentSpeed)
      if (networkStats.speedHistory.length > 10) {
        networkStats.speedHistory.shift()
      }

      const totalTime = (now - networkStats.startTime) / 1000
      speedInfo.average = totalTime > 0 ? uploadedBytes / totalTime : 0

      if (currentSpeed > speedInfo.peak) {
        speedInfo.peak = currentSpeed
      }

      networkStats.lastMeasureTime = now
      networkStats.lastMeasureBytes = uploadedBytes
    }
  }

  const resetNetworkStats = () => {
    speedInfo.current = 0
    speedInfo.average = 0
    speedInfo.peak = 0
    speedInfo.lastUpdate = 0
    networkStats.uploadedBytes = 0
    networkStats.totalBytes = 0
    networkStats.startTime = 0
    networkStats.lastMeasureTime = 0
    networkStats.lastMeasureBytes = 0
    networkStats.speedHistory = []
  }

  // æ–‡ä»¶å“ˆå¸Œè®¡ç®—
  const calculateFileHash = async (file: File): Promise<string> => {
    return new Promise((resolve, reject) => {
      isCalculatingHash.value = true
      hashProgress.loaded = 0
      hashProgress.total = file.size
      hashProgress.percentage = 0

      const spark = new SparkMD5.ArrayBuffer()
      const totalChunks = Math.ceil(file.size / hashChunkSize)
      let currentChunkIndex = 0
      let processedBytes = 0

      const processNextChunk = () => {
        if (currentChunkIndex >= totalChunks) {
          const hash = spark.end()
          isCalculatingHash.value = false
          resolve(hash)
          return
        }

        const start = currentChunkIndex * hashChunkSize
        const end = Math.min(start + hashChunkSize, file.size)
        const chunkBlob = file.slice(start, end)

        const fileReader = new FileReader()

        fileReader.addEventListener('load', e => {
          try {
            if (e.target?.result) {
              spark.append(e.target.result as ArrayBuffer)
              processedBytes += chunkBlob.size
              currentChunkIndex++

              hashProgress.loaded = processedBytes
              hashProgress.percentage = Math.round((processedBytes / file.size) * 100)

              setTimeout(processNextChunk, 0)
            }
          } catch {
            isCalculatingHash.value = false
            reject(new Error('MD5è®¡ç®—å¤±è´¥'))
          }
        })

        fileReader.onerror = () => {
          isCalculatingHash.value = false
          reject(new Error('æ–‡ä»¶è¯»å–å¤±è´¥'))
        }

        fileReader.readAsArrayBuffer(chunkBlob)
      }

      processNextChunk()
    })
  }

  // è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
  const startCalculateHash = async (): Promise<void> => {
    if (!currentFile.value) {
      throw new Error('è¯·å…ˆé€‰æ‹©æ–‡ä»¶')
    }

    try {
      console.log('=== å¼€å§‹è®¡ç®—S3æ–‡ä»¶å“ˆå¸Œ ===')
      const calculatedHash = await calculateFileHash(currentFile.value)
      fileHash.value = calculatedHash
      await checkUploadStatus()
      console.log('=== S3æ–‡ä»¶å“ˆå¸Œè®¡ç®—å®Œæˆ ===')
    } catch (error) {
      console.error('=== S3æ–‡ä»¶å“ˆå¸Œè®¡ç®—å¤±è´¥ ===', error)
      throw error
    }
  }

  // æ£€æŸ¥ä¸Šä¼ çŠ¶æ€
  const checkUploadStatus = async (): Promise<S3CheckUploadResponse> => {
    if (!currentFile.value || !fileHash.value) {
      throw new Error('æ–‡ä»¶æˆ–å“ˆå¸Œå€¼ä¸å­˜åœ¨')
    }

    try {
      isCheckingUpload.value = true
      console.log('ğŸ” æ£€æŸ¥S3æ–‡ä»¶ä¸Šä¼ çŠ¶æ€...')

      const response = await fetch(`${baseUrl}/check`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          ...headers
        },
        body: JSON.stringify({
          fileHash: fileHash.value,
          fileName: currentFile.value.name,
          fileSize: currentFile.value.size
        })
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const result: S3CheckUploadResponse = await response.json()

      if (result.fileExists && result.isComplete) {
        // S3ç§’ä¼ æˆåŠŸ
        console.log('âš¡ S3ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶å·²å­˜åœ¨')
        isSecondTransfer.value = true
        isCompleted.value = true
        return result
      } else if (result.session) {
        // å‘ç°æ–­ç‚¹ç»­ä¼ ä¼šè¯
        console.log('ğŸ”„ å‘ç°S3æ–­ç‚¹ç»­ä¼ ä¼šè¯')
        uploadSession.value = result.session
        resumeInfo.value = {
          progress: result.session.progress || 0,
          totalChunks: result.session.totalChunks || 0,
          uploadedCount: result.session.uploadedParts || 0
        }
        createChunksAndMarkUploaded(currentFile.value, result.session)
      }

      return result
    } catch (error) {
      console.error('æ£€æŸ¥S3ä¸Šä¼ çŠ¶æ€å¤±è´¥:', error)
      throw error
    } finally {
      isCheckingUpload.value = false
    }
  }

  // åˆ›å»ºåˆ†ç‰‡å¹¶æ ‡è®°å·²ä¸Šä¼ çš„åˆ†ç‰‡
  const createChunksAndMarkUploaded = async (file: File, session: S3UploadSession) => {
    const fileChunks: S3ChunkInfo[] = []
    const totalChunks = Math.ceil(file.size / chunkSize)

    for (let i = 0; i < totalChunks; i++) {
      const start = i * chunkSize
      const end = Math.min(start + chunkSize, file.size)
      const blob = file.slice(start, end)
      const partNumber = i + 1 // S3åˆ†ç‰‡å·ä»1å¼€å§‹

      const uploadedPartsCount = session.uploadedParts || 0
      const isUploaded = uploadedPartsCount > i

      // ä¸ºæ¯ä¸ªåˆ†ç‰‡è®¡ç®—å“ˆå¸Œ
      const chunkHash = await calculateChunkHash(blob)

      fileChunks.push({
        index: i,
        partNumber,
        start,
        end,
        blob,
        uploaded: isUploaded,
        progress: isUploaded ? 100 : 0,
        retryCount: 0,
        chunkHash
      })
    }

    chunks.value = fileChunks
    updateTotalProgress()
    console.log(`S3åˆ†ç‰‡åˆ›å»ºå®Œæˆï¼Œæ€»æ•°: ${fileChunks.length}ï¼Œå·²ä¸Šä¼ : ${session.uploadedParts}`)
  }

  // åˆ›å»ºåˆ†ç‰‡
  const createChunks = async (file: File): Promise<S3ChunkInfo[]> => {
    const fileChunks: S3ChunkInfo[] = []
    const totalChunks = Math.ceil(file.size / chunkSize)

    console.log(`åˆ›å»ºS3åˆ†ç‰‡ï¼Œåˆ†ç‰‡å¤§å°: ${(chunkSize / 1024 / 1024).toFixed(2)}MBï¼Œæ€»æ•°: ${totalChunks}`)

    for (let i = 0; i < totalChunks; i++) {
      const start = i * chunkSize
      const end = Math.min(start + chunkSize, file.size)
      const blob = file.slice(start, end)
      const partNumber = i + 1 // S3åˆ†ç‰‡å·ä»1å¼€å§‹

      // ä¸ºæ¯ä¸ªåˆ†ç‰‡è®¡ç®—å“ˆå¸Œ
      const chunkHash = await calculateChunkHash(blob)

      fileChunks.push({
        index: i,
        partNumber,
        start,
        end,
        blob,
        uploaded: false,
        progress: 0,
        retryCount: 0,
        chunkHash
      })
    }

    console.log(`S3åˆ†ç‰‡åˆ›å»ºå®Œæˆï¼Œæ€»æ•°: ${fileChunks.length}`)
    return fileChunks
  }

  // åˆå§‹åŒ–S3åˆ†ç‰‡ä¸Šä¼ 
  const initializeUpload = async (): Promise<S3UploadSession> => {
    if (!currentFile.value || !fileHash.value) {
      throw new Error('æ–‡ä»¶æˆ–å“ˆå¸Œå€¼ä¸å­˜åœ¨')
    }

    try {
      isInitializing.value = true
      console.log('ğŸš€ åˆå§‹åŒ–S3åˆ†ç‰‡ä¸Šä¼ ...')

      const response = await fetch(`${baseUrl}/initialize`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          ...headers
        },
        body: JSON.stringify({
          fileName: currentFile.value.name,
          fileHash: fileHash.value,
          fileSize: currentFile.value.size,
          chunkSize,
          totalChunks: Math.ceil(currentFile.value.size / chunkSize)
        })
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const result = await response.json()
      console.log(`S3åˆ†ç‰‡ä¸Šä¼ åˆå§‹åŒ–ç»“æœ: ï¼Œ`, result)

      if (!result.success) {
        throw new Error(result.message || 'S3åˆå§‹åŒ–å¤±è´¥')
      }

      console.log('âœ… S3åˆ†ç‰‡ä¸Šä¼ åˆå§‹åŒ–å®Œæˆ')
      return result.session
    } catch (error) {
      console.error('âŒ S3åˆå§‹åŒ–å¤±è´¥:', error)
      throw error
    } finally {
      isInitializing.value = false
    }
  }

  // ä¸Šä¼ å•ä¸ªåˆ†ç‰‡ - ä¿®æ”¹å‚æ•°ä»¥åŒ¹é…åç«¯æ¥å£
  const uploadChunk = async (chunk: S3ChunkInfo): Promise<boolean> => {
    if (!uploadSession.value || !chunk.chunkHash) {
      throw new Error('ä¸Šä¼ ä¼šè¯æˆ–åˆ†ç‰‡å“ˆå¸Œä¸å­˜åœ¨')
    }

    // åˆ›å»ºæ–°çš„ AbortController
    chunk.abortController = new AbortController()

    const formData = new FormData()
    formData.append('chunk', chunk.blob)
    formData.append('fileHash', fileHash.value)
    formData.append('chunkIndex', chunk.index.toString()) // åç«¯æœŸæœ›çš„æ˜¯ chunkIndex è€Œä¸æ˜¯ partNumber
    formData.append('chunkHash', chunk.chunkHash)

    try {
      chunk.progress = 1
      chunk.uploadStartTime = Date.now()

      const xhr = new XMLHttpRequest()

      return new Promise((resolve, reject) => {
        const timeoutId = setTimeout(() => {
          if (!chunk.abortController?.signal.aborted) {
            xhr.abort()
            reject(new Error('ä¸Šä¼ è¶…æ—¶'))
          }
        }, 60_000 * 60)

        // ç›‘å¬ AbortController ä¿¡å·
        if (chunk.abortController) {
          chunk.abortController.signal.addEventListener('abort', () => {
            clearTimeout(timeoutId)
            xhr.abort()
            reject(new Error('ä¸Šä¼ å·²å–æ¶ˆ'))
          })
        }

        xhr.upload.addEventListener('progress', event => {
          if (event.lengthComputable && !chunk.abortController?.signal.aborted) {
            chunk.progress = Math.round((event.loaded / event.total) * 100)

            const currentUploaded = uploadedSize.value + event.loaded
            updateNetworkStats(currentUploaded)
            updateTotalProgress()
          }
        })

        xhr.addEventListener('load', () => {
          clearTimeout(timeoutId)

          if (xhr.status >= 200 && xhr.status < 300) {
            try {
              const result = JSON.parse(xhr.responseText)

              if (result.success) {
                chunk.uploaded = true
                chunk.progress = 100
                chunk.etag = result.data?.etag
                chunk.uploadEndTime = Date.now()

                updateTotalProgress()
                console.log(`âœ… S3åˆ†ç‰‡ ${chunk.partNumber} ä¸Šä¼ æˆåŠŸ (ETag: ${result.data?.etag})`)
                resolve(true)
              } else {
                reject(new Error(result.message || 'S3åˆ†ç‰‡ä¸Šä¼ å¤±è´¥'))
              }
            } catch {
              reject(new Error('å“åº”è§£æå¤±è´¥'))
            }
          } else {
            reject(new Error(`HTTP ${xhr.status}: ${xhr.statusText}`))
          }
        })

        xhr.addEventListener('error', () => {
          clearTimeout(timeoutId)
          reject(new Error('ç½‘ç»œé”™è¯¯'))
        })

        xhr.addEventListener('abort', () => {
          clearTimeout(timeoutId)
          reject(new Error('è¯·æ±‚å·²ä¸­æ­¢'))
        })

        xhr.open('POST', `${baseUrl}/chunk`)
        Object.entries(headers).forEach(([key, value]) => {
          xhr.setRequestHeader(key, value)
        })

        xhr.send(formData)
      })
    } catch (error) {
      console.error(`S3åˆ†ç‰‡ ${chunk.partNumber} ä¸Šä¼ å¤±è´¥:`, error)
      chunk.retryCount++
      chunk.progress = 0
      return false
    }
  }

  // æ›´æ–°æ€»ä½“è¿›åº¦
  const updateTotalProgress = () => {
    if (!currentFile.value) return

    const uploaded = uploadedSize.value
    const total = currentFile.value.size

    uploadProgress.loaded = uploaded
    uploadProgress.total = total
    uploadProgress.percentage = total > 0 ? Math.round((uploaded / total) * 100) : 0
  }

  // å¹¶å‘ä¸Šä¼ åˆ†ç‰‡
  const uploadChunksConcurrently = async (): Promise<void> => {
    // åªå¤„ç†æœªä¸Šä¼ ä¸”æœªæ­£åœ¨æ‰§è¡Œçš„åˆ†ç‰‡
    const pendingChunks = remainingChunks.value.filter(chunk => !executingChunks.value.has(chunk.index))
    const executing: Promise<void>[] = []

    console.log(`å¼€å§‹å¹¶å‘ä¸Šä¼  ${pendingChunks.length} ä¸ªS3åˆ†ç‰‡ï¼Œå¹¶å‘æ•°: ${concurrent}`)

    const uploadSingleChunk = async (chunk: S3ChunkInfo): Promise<void> => {
      if (isPaused.value) return

      // æ ‡è®°åˆ†ç‰‡æ­£åœ¨æ‰§è¡Œ
      executingChunks.value.add(chunk.index)

      try {
        let success = false
        let attempts = 0

        while (!success && attempts < retryTimes && !isPaused.value) {
          attempts++
          success = await uploadChunk(chunk)

          if (!success && attempts < retryTimes && !isPaused.value) {
            console.log(`S3åˆ†ç‰‡ ${chunk.partNumber} ç¬¬ ${attempts} æ¬¡é‡è¯•`)
            await new Promise(resolve => setTimeout(resolve, 1000 * attempts))
          }
        }

        if (!success && !isPaused.value) {
          throw new Error(`S3åˆ†ç‰‡ ${chunk.partNumber} ä¸Šä¼ å¤±è´¥ï¼Œå·²é‡è¯• ${retryTimes} æ¬¡`)
        }
      } catch (error) {
        // å¦‚æœæ˜¯å› ä¸ºæš‚åœè€Œä¸­æ­¢ï¼Œä¸æŠ›å‡ºé”™è¯¯
        if (isPaused.value && error instanceof Error && error.message.includes('å·²å–æ¶ˆ')) {
          console.log(`S3åˆ†ç‰‡ ${chunk.partNumber} å› æš‚åœè€Œä¸­æ­¢ä¸Šä¼ `)
          return
        }
        throw error
      } finally {
        // ä»æ‰§è¡Œåˆ—è¡¨ä¸­ç§»é™¤
        executingChunks.value.delete(chunk.index)
      }
    }

    for (const chunk of pendingChunks) {
      if (isPaused.value) break

      const promise = uploadSingleChunk(chunk)
        .then(() => {
          const index = executing.indexOf(promise)
          if (index > -1) {
            executing.splice(index, 1)
          }
        })
        .catch(error => {
          const index = executing.indexOf(promise)
          if (index > -1) {
            executing.splice(index, 1)
          }
          // å¦‚æœä¸æ˜¯å› ä¸ºæš‚åœè€Œå¤±è´¥ï¼Œé‡æ–°æŠ›å‡ºé”™è¯¯
          if (!isPaused.value) {
            throw error
          }
        })

      executing.push(promise)

      if (executing.length >= concurrent) {
        try {
          await Promise.race(executing)
        } catch (error) {
          // å¦‚æœæ˜¯å› ä¸ºæš‚åœè€Œå¤±è´¥ï¼Œåœæ­¢æ·»åŠ æ–°çš„åˆ†ç‰‡
          if (isPaused.value) {
            break
          }
          throw error
        }
      }
    }

    // ç­‰å¾…æ‰€æœ‰æ­£åœ¨æ‰§è¡Œçš„åˆ†ç‰‡å®Œæˆï¼ˆæˆ–è¢«ä¸­æ­¢ï¼‰
    await Promise.allSettled(executing)

    if (!isPaused.value) {
      console.log('æ‰€æœ‰S3åˆ†ç‰‡ä¸Šä¼ å®Œæˆ')
    } else {
      console.log('S3åˆ†ç‰‡ä¸Šä¼ å·²æš‚åœ')
    }
  }

  // å®Œæˆä¸Šä¼  - ä¿®æ”¹å‚æ•°ä»¥åŒ¹é…åç«¯æ¥å£
  const completeUpload = async (): Promise<null | string> => {
    if (!uploadSession.value) {
      throw new Error('ä¸Šä¼ ä¼šè¯ä¸å­˜åœ¨')
    }
    if (isPaused.value) {
      throw new Error('ä¸Šä¼ å·²æš‚åœï¼Œæ— æ³•å®Œæˆ')
    }

    try {
      console.log('ğŸ”— å®ŒæˆS3åˆ†ç‰‡ä¸Šä¼ ...')

      // åç«¯åªéœ€è¦ fileHashï¼Œå…¶ä»–ä¿¡æ¯ä»ä¼šè¯ä¸­è·å–
      const response = await fetch(`${baseUrl}/complete`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          ...headers
        },
        body: JSON.stringify({
          fileHash: fileHash.value
        })
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const result = await response.json()

      if (result.success) {
        console.log('âœ… S3æ–‡ä»¶ä¸Šä¼ å®Œæˆ')
        return result.url || null
      } else {
        throw new Error(result.message || 'S3å®Œæˆä¸Šä¼ å¤±è´¥')
      }
    } catch (error) {
      console.error('âŒ S3å®Œæˆä¸Šä¼ å¤±è´¥:', error)
      throw error
    }
  }

  // å¼€å§‹ä¸Šä¼ 
  const startUpload = async (): Promise<null | string> => {
    if (!currentFile.value) {
      throw new Error('è¯·å…ˆé€‰æ‹©æ–‡ä»¶')
    }

    if (!fileHash.value) {
      throw new Error('è¯·å…ˆè®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼')
    }

    try {
      console.log('=== å¼€å§‹S3ä¸Šä¼ æµç¨‹ ===')
      resetNetworkStats()
      networkStats.totalBytes = currentFile.value.size

      isUploading.value = true
      isCompleted.value = false
      isPaused.value = false
      isSecondTransfer.value = false

      // æ£€æŸ¥ä¸Šä¼ çŠ¶æ€
      const checkResult = await checkUploadStatus()

      if (checkResult.fileExists && checkResult.isComplete) {
        // S3ç§’ä¼ æˆåŠŸ
        isCompleted.value = true
        isUploading.value = false
        isSecondTransfer.value = true
        return checkResult.url || null
      }

      // å¦‚æœæ²¡æœ‰ä¼šè¯ï¼Œåˆ›å»ºæ–°çš„ä¸Šä¼ ä¼šè¯
      if (!uploadSession.value) {
        uploadSession.value = await initializeUpload()
        chunks.value = await createChunks(currentFile.value)
      }

      updateTotalProgress()

      // å¦‚æœæ‰€æœ‰åˆ†ç‰‡éƒ½å·²ä¸Šä¼ ï¼Œç›´æ¥å®Œæˆ
      if (uploadedChunks.value.length === totalChunks.value) {
        const result = await completeUpload()
        isCompleted.value = true
        isUploading.value = false
        return result
      }

      // ä¸Šä¼ å‰©ä½™åˆ†ç‰‡
      await uploadChunksConcurrently()

      // å®Œæˆä¸Šä¼ 
      const result = await completeUpload()
      isCompleted.value = true
      isUploading.value = false

      console.log('=== S3ä¸Šä¼ æµç¨‹å®Œæˆ ===')
      return result
    } catch (error) {
      console.error('=== S3ä¸Šä¼ æµç¨‹å¤±è´¥ ===', error)
      isUploading.value = false
      throw error
    }
  }

  // æš‚åœä¸Šä¼ 
  const pauseUpload = async () => {
    console.log('æš‚åœS3ä¸Šä¼ ')
    isPaused.value = true

    // ä¸­æ­¢æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„åˆ†ç‰‡ä¸Šä¼ 
    for (const chunk of chunks.value) {
      if (chunk.abortController && !chunk.uploaded && chunk.progress > 0) {
        console.log(`ä¸­æ­¢S3åˆ†ç‰‡ ${chunk.partNumber} çš„ä¸Šä¼ `)
        chunk.abortController.abort()
        chunk.progress = 0 // é‡ç½®è¿›åº¦
      }
    }

    // ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©æ‰€æœ‰è¯·æ±‚éƒ½èƒ½è¢«ä¸­æ­¢
    // await new Promise(resolve => setTimeout(resolve, 100))

    console.log('S3ä¸Šä¼ å·²æš‚åœï¼Œæ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚å·²ä¸­æ­¢')
  }

  // æ¢å¤ä¸Šä¼ 
  const resumeUpload = async (): Promise<null | string> => {
    if (!currentFile.value || isCompleted.value) {
      throw new Error('æ²¡æœ‰å¯æ¢å¤çš„S3ä¸Šä¼ ä»»åŠ¡')
    }

    console.log('æ¢å¤S3ä¸Šä¼ ')
    isPaused.value = false
    isUploading.value = true

    // æ¸…é™¤æ‰§è¡Œä¸­çš„åˆ†ç‰‡è®°å½•ï¼ˆè¿™äº›åˆ†ç‰‡å¯èƒ½å› ä¸ºæš‚åœè€Œä¸­æ–­äº†ï¼‰
    executingChunks.value.clear()

    // é‡ç½®è¢«ä¸­æ–­çš„åˆ†ç‰‡çŠ¶æ€
    for (const chunk of chunks.value) {
      if (!chunk.uploaded && chunk.progress > 0 && chunk.progress < 100) {
        chunk.progress = 0 // é‡ç½®æœªå®Œæˆåˆ†ç‰‡çš„è¿›åº¦
        chunk.abortController = undefined // æ¸…é™¤æ—§çš„ AbortController
      }
    }

    try {
      await uploadChunksConcurrently()
      console.log(uploadedChunks.value, totalChunks.value)

      // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†ç‰‡éƒ½å·²ä¸Šä¼ å®Œæˆ
      if (uploadedChunks.value.length === totalChunks.value) {
        const result = await completeUpload()
        isCompleted.value = true
        isUploading.value = false
        return result
      } else {
        // å¦‚æœè¿˜æœ‰æœªå®Œæˆçš„åˆ†ç‰‡ä½†æ²¡æœ‰åœ¨ä¸Šä¼ ï¼Œå¯èƒ½æ˜¯å› ä¸ºå†æ¬¡æš‚åœ
        isUploading.value = false
        return null
      }
    } catch (error) {
      isUploading.value = false
      throw error
    }
  }

  // å–æ¶ˆä¸Šä¼ 
  const cancelUpload = async () => {
    console.log('å–æ¶ˆS3ä¸Šä¼ ')
    isPaused.value = true
    isUploading.value = false
    isCalculatingHash.value = false
    isCheckingUpload.value = false

    // ä¸­æ­¢æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„åˆ†ç‰‡ä¸Šä¼ 
    for (const chunk of chunks.value) {
      if (chunk.abortController && !chunk.uploaded) {
        chunk.abortController.abort()
      }
    }

    // æ¸…é™¤æ‰§è¡Œä¸­çš„åˆ†ç‰‡è®°å½•
    executingChunks.value.clear()

    if (uploadSession.value) {
      try {
        await fetch(`${baseUrl}/cancel`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            ...headers
          },
          body: JSON.stringify({
            fileHash: fileHash.value
          })
        })
        console.log('âœ… S3ä¸Šä¼ ä¼šè¯å·²å–æ¶ˆ')
      } catch (error) {
        console.error('âŒ å–æ¶ˆS3ä¸Šä¼ ä¼šè¯å¤±è´¥:', error)
      }
    }

    resetNetworkStats()
  }

  // é‡ç½®çŠ¶æ€
  const reset = () => {
    console.log('é‡ç½®S3ä¸Šä¼ çŠ¶æ€')
    isUploading.value = false
    isPaused.value = false
    isCompleted.value = false
    isCalculatingHash.value = false
    isCheckingUpload.value = false
    isInitializing.value = false
    isSecondTransfer.value = false
    currentFile.value = null
    fileHash.value = ''
    chunks.value = []
    uploadSession.value = null
    resumeInfo.value = null
    executingChunks.value.clear() // æ¸…é™¤æ‰§è¡Œä¸­çš„åˆ†ç‰‡è®°å½•
    uploadProgress.loaded = 0
    uploadProgress.total = 0
    uploadProgress.percentage = 0
    hashProgress.loaded = 0
    hashProgress.total = 0
    hashProgress.percentage = 0
    resetNetworkStats()
  }

  return {
    // çŠ¶æ€
    isUploading,
    isPaused,
    isCompleted,
    isCalculatingHash,
    isCheckingUpload,
    isInitializing,
    isSecondTransfer,
    currentFile,
    fileHash,
    chunks,
    uploadSession,
    uploadProgress,
    hashProgress,
    resumeInfo,

    // ç½‘ç»œé€Ÿåº¦ç›¸å…³
    speedInfo,
    networkStats,
    formatSpeed,
    calculateRemainingTime,

    // è®¡ç®—å±æ€§
    uploadedChunks,
    remainingChunks,
    totalChunks,
    uploadedSize,

    // æ–¹æ³•
    startCalculateHash,
    checkUploadStatus,
    startUpload,
    pauseUpload,
    resumeUpload,
    cancelUpload,
    reset
  }
}
